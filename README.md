# A Brief History of Language Machines

This repository is the official companion to the blog series **"A Brief History of Language Machines"** — a deep technical and narrative exploration of the evolution of Natural Language Processing (NLP).

The series traces the journey from early rule-based systems to the rise of neural architectures like Transformers and large language models (LLMs). Each chapter covers a key milestone in NLP’s development, combining historical context, architectural deep dives, and hands-on implementations.

All notebooks are implemented in PyTorch or TensorFlow and are intended to be accessible through Google Colab.

---

## Series Index

Each chapter corresponds to a blog post and a notebook folder in this repository.

| Chapter | Topic | Blog Link | Notebook Folder |
|--------|-------|-----------|------------------|
| 0 | Introduction: Why This Series | [Read](https://your-blog-link.com) | — |
| 1 | Before the Machines Understood | [Coming Soon](#) | 01-rule-based-systems/ |
| 2 | The Statistical Turn | [TBD](#) | 02-statistical-models/ |
| 3 | Word Embeddings | [TBD](#) | 03-word-embeddings/ |
| 4 | Memory Machines: RNNs & LSTMs | [TBD](#) | 04-rnn-lstm/ |
| 5 | Attention and the Transformer | [TBD](#) | 05-transformers/ |
| 6 | Pretraining and the Rise of BERT | [TBD](#) | 06-bert/ |
| 7 | Foundation Models and GPT | [TBD](#) | 07-foundation-models/ |

This table will be updated with new links and chapters as the series progresses.

---

## How to Use

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/brief-history-of-language-machines.git
   ```

2. Open any notebook via Google Colab using the badges inside each notebook or directly in your browser.

3. Modify, experiment, and explore the architecture hands-on.

No setup required beyond a modern browser and an internet connection.

---

## Citation & Credits

If you use any content or code from this project, please cite the blog series and repository.

Original blog series by [Rithesh Kumar](https://your-link.com).
Inspired by the works of researchers cited in each chapter.
